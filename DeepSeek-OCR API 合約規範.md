# DeepSeek-OCR API åˆç´„è¦ç¯„ (Single Source of Truth)

**ç‰ˆæœ¬**: 2.0.0  
**æœ€å¾Œæ›´æ–°**: 2025-11-04

---

## ğŸ“‹ ç›®éŒ„

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [åŸºç¤é…ç½®](#åŸºç¤é…ç½®)
3. [API ç«¯é»](#api-ç«¯é»)
4. [è³‡æ–™çµæ§‹å®šç¾©](#è³‡æ–™çµæ§‹å®šç¾©)
5. [éŒ¯èª¤è™•ç†](#éŒ¯èª¤è™•ç†)
6. [ä½¿ç”¨ç¯„ä¾‹](#ä½¿ç”¨ç¯„ä¾‹)
7. [æ ¸å¿ƒåŠŸèƒ½å¯¦ä½œ](#æ ¸å¿ƒåŠŸèƒ½å¯¦ä½œ)
8. [æœ€ä½³å¯¦è¸](#æœ€ä½³å¯¦è¸)

---

## æ¦‚è¿°

DeepSeek-OCR API æä¾›æ–‡æª” OCR è­˜åˆ¥æœå‹™,æ”¯æŒåœ–ç‰‡å’Œ PDF æ–‡æª”çš„æ–‡å­—æå–ã€Markdown è½‰æ›åŠè¦–è¦ºå®šä½(Grounding)åŠŸèƒ½ã€‚

### æ ¸å¿ƒç‰¹æ€§
- âœ… å¤šç¨®åœ–ç‰‡æ ¼å¼æ”¯æŒ (JPG, PNG, JPEG)
- âœ… PDF å¤šé æ‰¹é‡è™•ç†
- âœ… æ™ºèƒ½åœ–ç‰‡è£åˆ‡èˆ‡åˆ†å¡Š (Dynamic Preprocessing)
- âœ… è¦–è¦ºå®šä½ (Grounding) - æå–é‚Šç•Œæ¡†èˆ‡å­åœ–ç‰‡
- âœ… æµå¼è¼¸å‡º (Streaming)
- âœ… æ‰¹é‡ä¸¦ç™¼è™•ç†
- âœ… N-gram é˜²é‡è¤‡æ©Ÿåˆ¶
- âœ… å¤šç¨®é è¨­æ¨¡å¼

---

## åŸºç¤é…ç½®

### æ¨¡å‹é…ç½®æ¨¡å¼

| æ¨¡å¼ | BASE_SIZE | IMAGE_SIZE | CROP_MODE | MIN_CROPS | MAX_CROPS | é©ç”¨å ´æ™¯ |
|------|-----------|------------|-----------|-----------|-----------|----------|
| Tiny | 512 | 512 | false | 2 | 6 | å¿«é€Ÿè™•ç†å°åœ– |
| Small | 640 | 640 | false | 2 | 6 | æ¨™æº–æ–‡æª” |
| Base | 1024 | 1024 | false | 2 | 6 | é«˜è³ªé‡æ–‡æª” |
| Large | 1280 | 1280 | false | 2 | 6 | è¶…é«˜è§£æåº¦ |
| **Gundam** | 1024 | 640 | **true** | 2 | 6 | å¤§å‹æ–‡æª”æ™ºèƒ½è£åˆ‡ (é»˜èª) |

### ç³»çµ±é…ç½®åƒæ•¸

```typescript
interface SystemConfig {
  // åœ–ç‰‡è™•ç†
  BASE_SIZE: 512 | 640 | 1024 | 1280;      // å…¨å±€è¦–åœ–å°ºå¯¸
  IMAGE_SIZE: 512 | 640 | 1024 | 1280;     // å±€éƒ¨è¦–åœ–å°ºå¯¸
  CROP_MODE: boolean;                      // æ˜¯å¦å•Ÿç”¨å‹•æ…‹è£åˆ‡
  MIN_CROPS: number;                       // æœ€å°è£åˆ‡å¡Šæ•¸, é»˜èª: 2
  MAX_CROPS: number;                       // æœ€å¤§è£åˆ‡å¡Šæ•¸, é»˜èª: 6
  
  // æ€§èƒ½é…ç½®
  MAX_CONCURRENCY: number;                 // æœ€å¤§ä¸¦ç™¼æ•¸, é»˜èª: 100
  NUM_WORKERS: number;                     // åœ–ç‰‡é è™•ç†å·¥ä½œè€…æ•¸, é»˜èª: 64
  
  // æ¨¡å‹é…ç½®
  MODEL_PATH: string;                      // é»˜èª: 'deepseek-ai/DeepSeek-OCR'
  GPU_MEMORY_UTILIZATION: number;          // é»˜èª: 0.9
  MAX_MODEL_LEN: number;                   // é»˜èª: 8192
  
  // æ¨è«–é…ç½®
  TEMPERATURE: number;                     // é»˜èª: 0.0
  MAX_TOKENS: number;                      // é»˜èª: 8192
  SKIP_REPEAT: boolean;                    // è·³éé‡è¤‡é é¢, é»˜èª: true
  
  // N-gram é˜²é‡è¤‡
  NGRAM_SIZE: number;                      // é»˜èª: 20-40
  WINDOW_SIZE: number;                     // é»˜èª: 50-90
  WHITELIST_TOKEN_IDS: Set;        // ç™½åå–® Token (å¦‚ , )
}
```

---

## API ç«¯é»

### 1. OCR åœ–ç‰‡è­˜åˆ¥ (åŒæ­¥)

**ç«¯é»**: `POST /api/v1/ocr/image`

**è«‹æ±‚æ ¼å¼**:
```typescript
interface OCRImageRequest {
  // åœ–ç‰‡ä¾†æº (ä¸‰é¸ä¸€)
  image_url?: string;           // åœ–ç‰‡ URL
  image_base64?: string;        // Base64 ç·¨ç¢¼åœ–ç‰‡
  image_path?: string;          // æœ¬åœ°è·¯å¾‘ (åƒ…æœå‹™å™¨å…§éƒ¨)
  
  // è™•ç†é¸é …
  prompt?: string;              // é»˜èª: '\nConvert the document to markdown.'
  mode?: 'tiny' | 'small' | 'base' | 'large' | 'gundam';
  
  // é«˜ç´šé¸é …
  crop_mode?: boolean;          // è¦†è“‹é»˜èªè£åˆ‡æ¨¡å¼
  max_crops?: number;           // æœ€å¤§è£åˆ‡æ•¸é‡ (2-9)
  skip_repeat?: boolean;        // è·³éé‡è¤‡å…§å®¹
  
  // Grounding é¸é …
  extract_bounding_boxes?: boolean;  // æå–é‚Šç•Œæ¡†åº§æ¨™
  extract_sub_images?: boolean;      // æå–å­åœ–ç‰‡
  draw_bounding_boxes?: boolean;     // ç¹ªè£½é‚Šç•Œæ¡†
  
  // å…ƒæ•¸æ“š
  request_id?: string;
}
```

**å›æ‡‰æ ¼å¼**:
```typescript
interface OCRImageResponse {
  success: boolean;
  request_id: string;
  data: {
    // æ–‡å­—å…§å®¹
    text: string;                    // æå–çš„æ–‡å­—å…§å®¹
    markdown?: string;               // Markdown æ ¼å¼ (ç§»é™¤ grounding æ¨™è¨˜)
    text_with_grounding?: string;    // åŒ…å« grounding æ¨™è¨˜çš„åŸå§‹è¼¸å‡º
    
    // Grounding çµæœ
    grounding?: {
      bounding_boxes: Array<{
        label: string;               // æ¨™ç±¤é¡å‹ (å¦‚ 'title', 'image', 'table')
        coordinates: number[][];     // [[x1,y1,x2,y2], ...] æ­¸ä¸€åŒ–åº§æ¨™ (0-999)
        absolute_coordinates?: number[][];  // çµ•å°åº§æ¨™ (åƒç´ )
      }>;
      sub_images?: Array<{
        index: number;
        label: string;
        base64?: string;             // å­åœ–ç‰‡ Base64
        url?: string;                // å­åœ–ç‰‡ URL
      }>;
      visualization?: {
        image_with_boxes_base64?: string;  // ç¹ªè£½é‚Šç•Œæ¡†çš„åœ–ç‰‡
        image_with_boxes_url?: string;
      };
    };
    
    // è™•ç†ä¿¡æ¯
    processing_info: {
      mode: string;
      crop_enabled: boolean;
      num_crops: number;
      num_visual_tokens: number;
      processing_time_ms: number;
      
      // è£åˆ‡ä¿¡æ¯
      crop_ratio?: [number, number];  // [width_tiles, height_tiles]
    };
    
    // åœ–ç‰‡ä¿¡æ¯
    image_info: {
      width: number;
      height: number;
      format: string;
      size_bytes: number;
    };
  };
  timestamp: string;
}
```

### 2. OCR åœ–ç‰‡è­˜åˆ¥ (æµå¼)

**ç«¯é»**: `POST /api/v1/ocr/image/stream`

**è«‹æ±‚æ ¼å¼**: åŒä¸Š

**å›æ‡‰æ ¼å¼**: Server-Sent Events (SSE)
```typescript
// äº‹ä»¶é¡å‹
type StreamEvent = 
  | { type: 'start', data: { request_id: string } }
  | { type: 'token', data: { text: string, cumulative_text: string } }
  | { type: 'complete', data: OCRImageResponse }
  | { type: 'error', data: ErrorResponse };

// SSE æ ¼å¼
// data: {"type":"token","data":{"text":"#","cumulative_text":"#"}}
// data: {"type":"token","data":{"text":" Title","cumulative_text":"# Title"}}
// data: {"type":"complete","data":{...}}
```

### 3. OCR PDF è­˜åˆ¥

**ç«¯é»**: `POST /api/v1/ocr/pdf`

**è«‹æ±‚æ ¼å¼**:
```typescript
interface OCRPDFRequest {
  // PDF ä¾†æº (ä¸‰é¸ä¸€)
  pdf_url?: string;
  pdf_base64?: string;
  pdf_path?: string;
  
  // è™•ç†é¸é …
  prompt?: string;
  mode?: 'tiny' | 'small' | 'base' | 'large' | 'gundam';
  
  // PDF ç‰¹å®šé¸é …
  page_range?: {
    start: number;              // èµ·å§‹é ç¢¼ (1-based)
    end?: number;               // çµæŸé ç¢¼
  };
  pages?: number[];             // æŒ‡å®šé ç¢¼åˆ—è¡¨
  dpi?: number;                 // PDF è½‰åœ–ç‰‡ DPI, é»˜èª: 144
  
  // é«˜ç´šé¸é …
  crop_mode?: boolean;
  max_crops?: number;
  skip_repeat?: boolean;        // è·³éé‡è¤‡é é¢ (EOS æª¢æ¸¬)
  
  // Grounding é¸é …
  extract_bounding_boxes?: boolean;
  extract_sub_images?: boolean;
  draw_bounding_boxes?: boolean;
  generate_annotated_pdf?: boolean;  // ç”Ÿæˆæ¨™è¨» PDF
  
  // å…ƒæ•¸æ“š
  request_id?: string;
}
```

**å›æ‡‰æ ¼å¼**:
```typescript
interface OCRPDFResponse {
  success: boolean;
  request_id: string;
  data: {
    pages: Array<{
      page_number: number;
      text: string;
      markdown?: string;
      skipped?: boolean;         // æ˜¯å¦å› é‡è¤‡è€Œè·³é
      skip_reason?: string;      // 'no_eos' | 'duplicate'
      
      grounding?: {
        bounding_boxes: Array;
        sub_images?: Array;
      };
      
      processing_info: {
        mode: string;
        num_crops: number;
        num_visual_tokens: number;
        processing_time_ms: number;
      };
    }>;
    
    // æ•´é«”ä¿¡æ¯
    summary: {
      total_pages: number;
      processed_pages: number;
      skipped_pages: number;
      total_processing_time_ms: number;
      total_text_length: number;
    };
    
    // åˆä½µå…§å®¹
    merged_content: {
      markdown: string;          // æ‰€æœ‰é é¢åˆä½µçš„ Markdown
      markdown_with_separators: string;  // å¸¶  åˆ†éš”ç¬¦
    };
    
    // PDF ä¿¡æ¯
    pdf_info: {
      page_count: number;
      file_size_bytes: number;
    };
    
    // é™„ä»¶ (å¦‚æœè«‹æ±‚)
    attachments?: {
      annotated_pdf_base64?: string;     // æ¨™è¨»é‚Šç•Œæ¡†çš„ PDF
      annotated_pdf_url?: string;
    };
  };
  timestamp: string;
}
```

### 4. æ‰¹é‡è™•ç†

**ç«¯é»**: `POST /api/v1/ocr/batch`

**è«‹æ±‚æ ¼å¼**:
```typescript
interface OCRBatchRequest {
  items: Array<{
    id: string;                 // é …ç›®å”¯ä¸€ ID
    type: 'image' | 'pdf';
    source: string;             // URL æˆ– Base64
    source_type: 'url' | 'base64' | 'path';
    prompt?: string;
    mode?: string;
    
    // å¯è¦†è“‹å…¨å±€é¸é …
    crop_mode?: boolean;
    max_crops?: number;
    extract_bounding_boxes?: boolean;
  }>;
  
  // æ‰¹é‡é¸é …
  batch_options?: {
    max_concurrent?: number;    // æœ€å¤§ä¸¦ç™¼æ•¸, é»˜èª: ä½¿ç”¨ç³»çµ±é…ç½®
    fail_fast?: boolean;        // é‡éŒ¯å³åœ, é»˜èª: false
    num_workers?: number;       // é è™•ç†å·¥ä½œè€…æ•¸
  };
  
  request_id?: string;
}
```

**å›æ‡‰æ ¼å¼**:
```typescript
interface OCRBatchResponse {
  success: boolean;
  request_id: string;
  data: {
    results: Array;
    
    summary: {
      total: number;
      succeeded: number;
      failed: number;
      total_processing_time_ms: number;
      average_processing_time_ms: number;
    };
  };
  timestamp: string;
}
```

### 5. å¥åº·æª¢æŸ¥

**ç«¯é»**: `GET /api/v1/health`

**å›æ‡‰æ ¼å¼**:
```typescript
interface HealthResponse {
  status: 'healthy' | 'degraded' | 'unhealthy';
  version: string;
  model: {
    loaded: boolean;
    path: string;
    mode: string;
    architecture: string;      // 'DeepseekOCRForCausalLM'
  };
  system: {
    gpu_available: boolean;
    gpu_count: number;
    gpu_memory_used_mb?: number;
    gpu_memory_total_mb?: number;
    gpu_utilization?: number;
    current_concurrency: number;
    max_concurrency: number;
  };
  config: {
    base_size: number;
    image_size: number;
    crop_mode: boolean;
    max_crops: number;
  };
  timestamp: string;
}
```

### 6. é…ç½®ç®¡ç†

**ç«¯é»**: `GET /api/v1/config`

**å›æ‡‰æ ¼å¼**:
```typescript
interface ConfigResponse {
  current_mode: string;
  available_modes: string[];
  config: SystemConfig;
  prompt_templates: {
    document: string;
    image: string;
    figure: string;
    general: string;
    free_ocr: string;
    table: string;
    form: string;
    recognition: string;
  };
  ngram_config: {
    ngram_size: number;
    window_size: number;
    whitelist_token_ids: number[];
  };
}
```

**ç«¯é»**: `PUT /api/v1/config`

**è«‹æ±‚æ ¼å¼**:
```typescript
interface ConfigUpdateRequest {
  mode?: 'tiny' | 'small' | 'base' | 'large' | 'gundam';
  max_concurrency?: number;
  max_crops?: number;
  skip_repeat?: boolean;
  ngram_config?: {
    ngram_size?: number;
    window_size?: number;
  };
}
```

---

## è³‡æ–™çµæ§‹å®šç¾©

### æç¤ºè©æ¨¡æ¿

```typescript
enum PromptTemplate {
  // æ–‡æª”è™•ç†
  DOCUMENT = '\nConvert the document to markdown.',
  IMAGE = '\nOCR this image.',
  FREE_OCR = '\nFree OCR.',  // ç„¡ grounding æ¨™è¨˜
  
  // ç‰¹æ®Šå…§å®¹
  FIGURE = '\nParse the figure.',
  TABLE = '\nExtract table data in markdown format.',
  FORM = '\nExtract form fields and values.',
  
  // é€šç”¨
  GENERAL = '\nDescribe this image in detail.',
  RECOGNITION = '\nLocate {target}<|/ref|> in the image.',
  
  // ç§‘å­¸å…§å®¹
  CHEMISTRY = '\nExtract the structural formula.',  // è¼¸å‡º SMILES
  GEOMETRY = '\nExtract geometric data.',           // è¼¸å‡ºåº§æ¨™
}
```

### Grounding æ¨™è¨˜æ ¼å¼

```typescript
// è¼¸å‡ºæ ¼å¼
type GroundingOutput = string;  // 'label<|/ref|>[[x1,y1,x2,y2],...]<|/det|>'

// è§£æå¾Œæ ¼å¼
interface GroundingData {
  label: string;                // 'image' | 'title' | 'table' | 'figure' | ...
  coordinates: number[][];      // [[x1,y1,x2,y2], ...], æ­¸ä¸€åŒ– 0-999
}

// æ­£å‰‡è¡¨é”å¼
const GROUNDING_PATTERN = /((.*?)<\|\/ref\|>(.*?)<\|\/det\|>)/g;
```

### è™•ç†æ¨¡å¼

```typescript
enum ProcessingMode {
  TINY = 'tiny',
  SMALL = 'small',
  BASE = 'base',
  LARGE = 'large',
  GUNDAM = 'gundam'
}

// æ¨¡å¼é…ç½®æ˜ å°„
const MODE_CONFIG: Record = {
  tiny: { base_size: 512, image_size: 512, crop_mode: false },
  small: { base_size: 640, image_size: 640, crop_mode: false },
  base: { base_size: 1024, image_size: 1024, crop_mode: false },
  large: { base_size: 1280, image_size: 1280, crop_mode: false },
  gundam: { base_size: 1024, image_size: 640, crop_mode: true },
};
```

### åœ–ç‰‡é è™•ç†

```typescript
interface ImagePreprocessResult {
  // è¼¸å…¥
  input_ids: torch.LongTensor;           // [seq_len]
  
  // åœ–ç‰‡ç‰¹å¾µ
  pixel_values: torch.FloatTensor;       // [n_images, 3, base_size, base_size]
  images_crop: torch.FloatTensor;        // [n_images, n_patches, 3, image_size, image_size]
  images_seq_mask: torch.BoolTensor;     // [seq_len], æ¨™è¨˜åœ–ç‰‡ token ä½ç½®
  images_spatial_crop: torch.LongTensor; // [n_images, 2], [width_tiles, height_tiles]
  
  // å…ƒæ•¸æ“š
  num_image_tokens: number[];            // æ¯å¼µåœ–ç‰‡çš„ token æ•¸é‡
  image_shapes: [number, number][];      // åŸå§‹åœ–ç‰‡å°ºå¯¸ (width, height)
}
```

---

## éŒ¯èª¤è™•ç†

### æ¨™æº–éŒ¯èª¤æ ¼å¼

```typescript
interface ErrorResponse {
  success: false;
  error: {
    code: string;
    message: string;
    details?: any;
    request_id?: string;
    traceback?: string;  // åƒ…é–‹ç™¼ç’°å¢ƒ
  };
  timestamp: string;
}
```

### éŒ¯èª¤ä»£ç¢¼

| ä»£ç¢¼ | HTTP ç‹€æ…‹ | èªªæ˜ | è§£æ±ºæ–¹æ¡ˆ |
|------|-----------|------|----------|
| `INVALID_INPUT` | 400 | è«‹æ±‚åƒæ•¸ç„¡æ•ˆ | æª¢æŸ¥è«‹æ±‚æ ¼å¼ |
| `MISSING_IMAGE` | 400 | æœªæä¾›åœ–ç‰‡ä¾†æº | æä¾› image_url/base64/path |
| `UNSUPPORTED_FORMAT` | 400 | ä¸æ”¯æŒçš„åœ–ç‰‡æ ¼å¼ | ä½¿ç”¨ JPG/PNG/JPEG |
| `INVALID_PROMPT` | 400 | æç¤ºè©æ ¼å¼éŒ¯èª¤ | ç¢ºä¿åŒ…å« `<image>` æ¨™è¨˜ |
| `IMAGE_TOO_LARGE` | 413 | åœ–ç‰‡éå¤§ | å£“ç¸®åœ–ç‰‡æˆ–èª¿æ•´æ¨¡å¼ |
| `PDF_CONVERSION_FAILED` | 422 | PDF è½‰åœ–ç‰‡å¤±æ•— | æª¢æŸ¥ PDF æª”æ¡ˆ |
| `MODEL_NOT_LOADED` | 503 | æ¨¡å‹æœªåŠ è¼‰ | ç­‰å¾…æ¨¡å‹åˆå§‹åŒ– |
| `GPU_OOM` | 503 | GPU è¨˜æ†¶é«”ä¸è¶³ | é™ä½ MAX_CROPS æˆ–ä¸¦ç™¼æ•¸ |
| `PROCESSING_TIMEOUT` | 504 | è™•ç†è¶…æ™‚ | ç°¡åŒ–åœ–ç‰‡æˆ–åˆ†æ‰¹è™•ç† |
| `RATE_LIMIT_EXCEEDED` | 429 | è¶…éä¸¦ç™¼é™åˆ¶ | é™ä½è«‹æ±‚é »ç‡ |
| `INFERENCE_ERROR` | 500 | æ¨è«–å¤±æ•— | æª¢æŸ¥æ—¥èªŒ |

---

## ä½¿ç”¨ç¯„ä¾‹

### Python å®¢æˆ¶ç«¯å®Œæ•´å¯¦ä½œ

```python
import requests
import base64
from typing import Optional, List, Dict, Any
from enum import Enum

class PromptTemplate(Enum):
    DOCUMENT = '\\nConvert the document to markdown.'
    IMAGE = '\\nOCR this image.'
    FREE_OCR = '\\nFree OCR.'
    FIGURE = '\\nParse the figure.'
    TABLE = '\\nExtract table data in markdown format.'

class ProcessingMode(Enum):
    TINY = 'tiny'
    SMALL = 'small'
    BASE = 'base'
    LARGE = 'large'
    GUNDAM = 'gundam'

class DeepSeekOCRClient:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session = requests.Session()
    
    def _load_image_as_base64(self, image_path: str) -> str:
        """è¼‰å…¥åœ–ç‰‡ä¸¦è½‰ç‚º Base64"""
        with open(image_path, "rb") as f:
            return base64.b64encode(f.read()).decode()
    
    def ocr_image(
        self,
        image_path: Optional[str] = None,
        image_url: Optional[str] = None,
        image_base64: Optional[str] = None,
        prompt: str = PromptTemplate.DOCUMENT.value,
        mode: str = ProcessingMode.GUNDAM.value,
        extract_bounding_boxes: bool = False,
        extract_sub_images: bool = False,
        draw_bounding_boxes: bool = False,
        crop_mode: Optional[bool] = None,
        max_crops: Optional[int] = None,
    ) -> Dict[str, Any]:
        """OCR åœ–ç‰‡è­˜åˆ¥"""
        
        data = {
            "prompt": prompt,
            "mode": mode,
            "extract_bounding_boxes": extract_bounding_boxes,
            "extract_sub_images": extract_sub_images,
            "draw_bounding_boxes": draw_bounding_boxes,
        }
        
        if crop_mode is not None:
            data["crop_mode"] = crop_mode
        if max_crops is not None:
            data["max_crops"] = max_crops
        
        # åœ–ç‰‡ä¾†æº
        if image_path:
            data["image_base64"] = self._load_image_as_base64(image_path)
        elif image_url:
            data["image_url"] = image_url
        elif image_base64:
            data["image_base64"] = image_base64
        else:
            raise ValueError("å¿…é ˆæä¾› image_path, image_url æˆ– image_base64")
        
        response = self.session.post(
            f"{self.base_url}/api/v1/ocr/image",
            json=data,
            timeout=120
        )
        response.raise_for_status()
        return response.json()
    
    def ocr_image_stream(
        self,
        image_path: Optional[str] = None,
        image_url: Optional[str] = None,
        prompt: str = PromptTemplate.DOCUMENT.value,
        mode: str = ProcessingMode.GUNDAM.value,
    ):
        """OCR åœ–ç‰‡è­˜åˆ¥ (æµå¼è¼¸å‡º)"""
        
        data = {"prompt": prompt, "mode": mode}
        
        if image_path:
            data["image_base64"] = self._load_image_as_base64(image_path)
        elif image_url:
            data["image_url"] = image_url
        else:
            raise ValueError("å¿…é ˆæä¾› image_path æˆ– image_url")
        
        response = self.session.post(
            f"{self.base_url}/api/v1/ocr/image/stream",
            json=data,
            stream=True,
            timeout=120
        )
        response.raise_for_status()
        
        # è§£æ SSE
        for line in response.iter_lines():
            if line:
                line = line.decode('utf-8')
                if line.startswith('data: '):
                    import json
                    data = json.loads(line[6:])
                    yield data
    
    def ocr_pdf(
        self,
        pdf_path: Optional[str] = None,
        pdf_url: Optional[str] = None,
        page_range: Optional[Dict[str, int]] = None,
        pages: Optional[List[int]] = None,
        mode: str = ProcessingMode.GUNDAM.value,
        dpi: int = 144,
        skip_repeat: bool = True,
        extract_bounding_boxes: bool = False,
        generate_annotated_pdf: bool = False,
    ) -> Dict[str, Any]:
        """OCR PDF è­˜åˆ¥"""
        
        data = {
            "mode": mode,
            "dpi": dpi,
            "skip_repeat": skip_repeat,
            "extract_bounding_boxes": extract_bounding_boxes,
            "generate_annotated_pdf": generate_annotated_pdf,
        }
        
        if page_range:
            data["page_range"] = page_range
        if pages:
            data["pages"] = pages
        
        if pdf_path:
            with open(pdf_path, "rb") as f:
                data["pdf_base64"] = base64.b64encode(f.read()).decode()
        elif pdf_url:
            data["pdf_url"] = pdf_url
        else:
            raise ValueError("å¿…é ˆæä¾› pdf_path æˆ– pdf_url")
        
        response = self.session.post(
            f"{self.base_url}/api/v1/ocr/pdf",
            json=data,
            timeout=600  # PDF è™•ç†å¯èƒ½è¼ƒæ…¢
        )
        response.raise_for_status()
        return response.json()
    
    def batch_ocr(
        self,
        items: List[Dict[str, Any]],
        max_concurrent: Optional[int] = None,
        fail_fast: bool = False,
    ) -> Dict[str, Any]:
        """æ‰¹é‡ OCR"""
        
        data = {
            "items": items,
            "batch_options": {
                "fail_fast": fail_fast,
            }
        }
        
        if max_concurrent:
            data["batch_options"]["max_concurrent"] = max_concurrent
        
        response = self.session.post(
            f"{self.base_url}/api/v1/ocr/batch",
            json=data,
            timeout=1200
        )
        response.raise_for_status()
        return response.json()
    
    def health(self) -> Dict[str, Any]:
        """å¥åº·æª¢æŸ¥"""
        response = self.session.get(f"{self.base_url}/api/v1/health")
        return response.json()
    
    def get_config(self) -> Dict[str, Any]:
        """ç²å–é…ç½®"""
        response = self.session.get(f"{self.base_url}/api/v1/config")
        return response.json()

# ============ ä½¿ç”¨ç¯„ä¾‹ ============

client = DeepSeekOCRClient(base_url="http://localhost:8000")

# 1. åŸºç¤åœ–ç‰‡ OCR
result = client.ocr_image(
    image_path="document.jpg",
    prompt=PromptTemplate.DOCUMENT.value,
    mode=ProcessingMode.GUNDAM.value
)
print("Markdown:", result["data"]["markdown"])

# 2. å¸¶ Grounding çš„åœ–ç‰‡ OCR
result = client.ocr_image(
    image_path="document.jpg",
    prompt=PromptTemplate.DOCUMENT.value,
    extract_bounding_boxes=True,
    extract_sub_images=True,
    draw_bounding_boxes=True,
)

# æå–é‚Šç•Œæ¡†
for bbox in result["data"]["grounding"]["bounding_boxes"]:
    print(f"Label: {bbox['label']}, Coords: {bbox['coordinates']}")

# ä¿å­˜å­åœ–ç‰‡
for sub_img in result["data"]["grounding"]["sub_images"]:
    img_data = base64.b64decode(sub_img["base64"])
    with open(f"sub_image_{sub_img['index']}.jpg", "wb") as f:
        f.write(img_data)

# 3. æµå¼è¼¸å‡º
print("Streaming output:")
for event in client.ocr_image_stream(
    image_path="document.jpg",
    prompt=PromptTemplate.DOCUMENT.value
):
    if event["type"] == "token":
        print(event["data"]["text"], end="", flush=True)
    elif event["type"] == "complete":
        print("\n\nComplete!")

# 4. PDF OCR
pdf_result = client.ocr_pdf(
    pdf_path="report.pdf",
    page_range={"start": 1, "end": 5},
    mode=ProcessingMode.BASE.value,
    skip_repeat=True,
    generate_annotated_pdf=True,
)

print(f"Processed {pdf_result['data']['summary']['processed_pages']} pages")
print(f"Skipped {pdf_result['data']['summary']['skipped_pages']} pages")
print("Merged Markdown:", pdf_result["data"]["merged_content"]["markdown"])

# 5. æ‰¹é‡è™•ç†
items = [
    {
        "id": "img1",
        "type": "image",
        "source": "https://example.com/doc1.jpg",
        "source_type": "url",
        "mode": "gundam"
    },
    {
        "id": "img2",
        "type": "image",
        "source": "/path/to/local/doc2.jpg",
        "source_type": "path",
        "mode": "base"
    }
]

batch_result = client.batch_ocr(items, max_concurrent=10)
for item in batch_result["data"]["results"]:
    if item["success"]:
        print(f"{item['id']}: {item['result']['data']['markdown'][:100]}...")
    else:
        print(f"{item['id']}: Error - {item['error']['message']}")
```

### cURL ç¯„ä¾‹

```bash
# 1. åŸºç¤åœ–ç‰‡ OCR (ä½¿ç”¨ URL)
curl -X POST http://localhost:8000/api/v1/ocr/image \
  -H "Content-Type: application/json" \
  -d '{
    "image_url": "https://example.com/document.jpg",
    "prompt": "\\nConvert the document to markdown.",
    "mode": "gundam"
  }'

# 2. åœ–ç‰‡ OCR with Grounding
curl -X POST http://localhost:8000/api/v1/ocr/image \
  -H "Content-Type: application/json" \
  -d '{
    "image_base64": "iVBORw0KGgoAAAANS...",
    "prompt": "\\nConvert the document to markdown.",
    "extract_bounding_boxes": true,
    "extract_sub_images": true,
    "draw_bounding_boxes": true
  }'

# 3. æµå¼è¼¸å‡º
curl -X POST http://localhost:8000/api/v1/ocr/image/stream \
  -H "Content-Type: application/json" \
  -N \
  -d '{
    "image_url": "https://example.com/doc.jpg",
    "prompt": "\\nConvert the document to markdown."
  }'

# 4. PDF OCR
curl -X POST http://localhost:8000/api/v1/ocr/pdf \
  -H "Content-Type: application/json" \
  -d '{
    "pdf_url": "https://example.com/report.pdf",
    "page_range": {"start": 1, "end": 5},
    "mode": "base",
    "dpi": 144,
    "skip_repeat": true
  }'

# 5. å¥åº·æª¢æŸ¥
curl http://localhost:8000/api/v1/health

# 6. æŸ¥è©¢é…ç½®
curl http://localhost:8000/api/v1/config

# 7. æ›´æ–°é…ç½®
curl -X PUT http://localhost:8000/api/v1/config \
  -H "Content-Type: application/json" \
  -d '{
    "mode": "gundam",
    "max_concurrency": 50,
    "max_crops": 6
  }'
```

### TypeScript/JavaScript ç¯„ä¾‹

```typescript
interface OCRImageRequest {
  image_url?: string;
  image_base64?: string;
  prompt?: string;
  mode?: string;
  extract_bounding_boxes?: boolean;
  extract_sub_images?: boolean;
  draw_bounding_boxes?: boolean;
}

class DeepSeekOCRClient {
  constructor(private baseUrl: string = 'http://localhost:8000') {}
  
  async ocrImage(request: OCRImageRequest): Promise {
    const response = await fetch(`${this.baseUrl}/api/v1/ocr/image`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(request)
    });
    
    if (!response.ok) {
      const error = await response.json();
      throw new Error(`OCR failed: ${error.error.message}`);
    }
    
    return response.json();
  }
  
  async *ocrImageStream(request: OCRImageRequest): AsyncGenerator {
    const response = await fetch(`${this.baseUrl}/api/v1/ocr/image/stream`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(request)
    });
    
    if (!response.ok) {
      throw new Error(`Stream failed: ${response.statusText}`);
    }
    
    const reader = response.body!.getReader();
    const decoder = new TextDecoder();
    
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      
      const text = decoder.decode(value);
      const lines = text.split('\n');
      
      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = JSON.parse(line.slice(6));
          yield data;
        }
      }
    }
  }
  
  async ocrPDF(request: any): Promise {
    const response = await fetch(`${this.baseUrl}/api/v1/ocr/pdf`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(request)
    });
    
    if (!response.ok) {
      const error = await response.json();
      throw new Error(`PDF OCR failed: ${error.error.message}`);
    }
    
    return response.json();
  }
  
  async health(): Promise {
    const response = await fetch(`${this.baseUrl}/api/v1/health`);
    return response.json();
  }
}

// ä½¿ç”¨ç¯„ä¾‹
const client = new DeepSeekOCRClient();

// 1. åŸºç¤ OCR
const result = await client.ocrImage({
  image_url: 'https://example.com/doc.jpg',
  mode: 'gundam',
  prompt: '\\nConvert the document to markdown.'
});
console.log(result.data.markdown);

// 2. æµå¼è¼¸å‡º
for await (const event of client.ocrImageStream({
  image_url: 'https://example.com/doc.jpg'
})) {
  if (event.type === 'token') {
    process.stdout.write(event.data.text);
  } else if (event.type === 'complete') {
    console.log('\nDone!');
  }
}

// 3. PDF OCR
const pdfResult = await client.ocrPDF({
  pdf_url: 'https://example.com/report.pdf',
  page_range: { start: 1, end: 10 },
  mode: 'base'
});
console.log(pdfResult.data.merged_content.markdown);
```

---

## æ ¸å¿ƒåŠŸèƒ½å¯¦ä½œ

### 1. åœ–ç‰‡å‹•æ…‹è£åˆ‡ (Dynamic Preprocessing)

**æ¼”ç®—æ³•**: `dynamic_preprocess()`

```python
def dynamic_preprocess(
    image: PIL.Image,
    min_num: int = 2,
    max_num: int = 6,
    image_size: int = 640
) -> Tuple[List[PIL.Image], Tuple[int, int]]:
    """
    æ ¹æ“šåœ–ç‰‡é•·å¯¬æ¯”å‹•æ…‹è£åˆ‡æˆå¤šå€‹å¡Š
    
    Args:
        image: åŸå§‹åœ–ç‰‡
        min_num: æœ€å°è£åˆ‡å¡Šæ•¸
        max_num: æœ€å¤§è£åˆ‡å¡Šæ•¸
        image_size: æ¯å€‹å¡Šçš„å°ºå¯¸
    
    Returns:
        (è£åˆ‡å¾Œçš„åœ–ç‰‡åˆ—è¡¨, (width_tiles, height_tiles))
    """
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height
    
    # è¨ˆç®—æ‰€æœ‰å¯èƒ½çš„è£åˆ‡æ¯”ä¾‹
    target_ratios = set(
        (i, j) for n in range(min_num, max_num + 1)
        for i in range(1, n + 1)
        for j in range(1, n + 1)
        if i * j <= max_num and i * j >= min_num
    )
    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])
    
    # æ‰¾åˆ°æœ€æ¥è¿‘çš„è£åˆ‡æ¯”ä¾‹
    best_ratio = find_closest_aspect_ratio(
        aspect_ratio, target_ratios, orig_width, orig_height, image_size
    )
    
    # åŸ·è¡Œè£åˆ‡
    target_width = image_size * best_ratio[0]
    target_height = image_size * best_ratio[1]
    resized_img = image.resize((target_width, target_height))
    
    processed_images = []
    blocks = best_ratio[0] * best_ratio[1]
    
    for i in range(blocks):
        box = (
            (i % (target_width // image_size)) * image_size,
            (i // (target_width // image_size)) * image_size,
            ((i % (target_width // image_size)) + 1) * image_size,
            ((i // (target_width // image_size)) + 1) * image_size
        )
        split_img = resized_img.crop(box)
        processed_images.append(split_img)
    
    return processed_images, best_ratio
```

**ç¯„ä¾‹**:
- è¼¸å…¥: 1920Ã—1080 åœ–ç‰‡
- IMAGE_SIZE: 640
- è¼¸å‡º: 3Ã—2 = 6 å€‹ 640Ã—640 çš„å¡Š, crop_ratio=(3, 2)

### 2. N-gram é˜²é‡è¤‡æ©Ÿåˆ¶

**æ¼”ç®—æ³•**: `NoRepeatNGramLogitsProcessor`

```python
class NoRepeatNGramLogitsProcessor:
    """é˜²æ­¢ç”Ÿæˆé‡è¤‡çš„ N-gram"""
    
    def __init__(
        self,
        ngram_size: int = 20,
        window_size: int = 50,
        whitelist_token_ids: Set[int] = None
    ):
        self.ngram_size = ngram_size
        self.window_size = window_size
        self.whitelist_token_ids = whitelist_token_ids or set()
    
    def __call__(
        self,
        input_ids: List[int],
        scores: torch.FloatTensor
    ) -> torch.FloatTensor:
        if len(input_ids) < self.ngram_size:
            return scores
        
        # ç•¶å‰å‰ç¶´
        current_prefix = tuple(input_ids[-(self.ngram_size - 1):])
        
        # åœ¨çª—å£å…§æœç´¢é‡è¤‡çš„ n-gram
        search_start = max(0, len(input_ids) - self.window_size)
        search_end = len(input_ids) - self.ngram_size + 1
        
        banned_tokens = set()
        for i in range(search_start, search_end):
            ngram = tuple(input_ids[i:i + self.ngram_size])
            if ngram[:-1] == current_prefix:
                banned_tokens.add(ngram[-1])
        
        # ç™½åå–® token ä¸ç¦æ­¢ (å¦‚ , )
        banned_tokens = banned_tokens - self.whitelist_token_ids
        
        # å°‡ç¦æ­¢çš„ token åˆ†æ•¸è¨­ç‚º -inf
        if banned_tokens:
            scores = scores.clone()
            for token in banned_tokens:
                scores[token] = -float("inf")
        
        return scores
```

**é…ç½®å»ºè­°**:
- æ‰¹é‡è™•ç†: `ngram_size=40, window_size=90`
- å–®åœ–è™•ç†: `ngram_size=30, window_size=90`
- PDF è™•ç†: `ngram_size=20, window_size=50`
- ç™½åå–®: `{128821, 128822}` (å°æ‡‰ `<td>`, `</td>`)

### 3. Grounding è§£æèˆ‡è™•ç†

**è§£æ Grounding æ¨™è¨˜**:

```python
import re
from typing import List, Tuple

def parse_grounding(text: str) -> Tuple[List[dict], List[str], List[str]]:
    """
    è§£æ grounding æ¨™è¨˜
    
    Returns:
        (æ‰€æœ‰åŒ¹é…, åœ–ç‰‡é¡å‹åŒ¹é…, å…¶ä»–é¡å‹åŒ¹é…)
    """
    pattern = r'((.*?)<\|\/ref\|>(.*?)<\|\/det\|>)'
    matches = re.findall(pattern, text, re.DOTALL)
    
    matches_image = []
    matches_other = []
    
    for match in matches:
        if 'image<|/ref|>' in match[0]:
            matches_image.append(match[0])
        else:
            matches_other.append(match[0])
    
    parsed_matches = []
    for match in matches:
        try:
            label = match[1]
            coordinates = eval(match[2])  # [[x1,y1,x2,y2], ...]
            parsed_matches.append({
                'label': label,
                'coordinates': coordinates,
                'raw': match[0]
            })
        except:
            continue
    
    return parsed_matches, matches_image, matches_other

def remove_grounding_markers(text: str, matches_other: List[str]) -> str:
    """ç§»é™¤ grounding æ¨™è¨˜,ä¿ç•™ç´”æ–‡å­—"""
    for match in matches_other:
        text = text.replace(match, '')
    
    # æ¸…ç†å¤šé¤˜æ›è¡Œ
    text = text.replace('\n\n\n\n', '\n\n').replace('\n\n\n', '\n\n')
    text = text.replace('', '').replace('', '')
    text = text.replace('\\coloneqq', ':=').replace('\\eqqcolon', '=:')
    
    return text

def extract_coordinates(
    match: dict,
    image_width: int,
    image_height: int
) -> Tuple[str, List[List[int]]]:
    """
    å°‡æ­¸ä¸€åŒ–åº§æ¨™ (0-999) è½‰æ›ç‚ºçµ•å°åº§æ¨™
    
    Returns:
        (label, [[x1,y1,x2,y2], ...])
    """
    label = match['label']
    coords_normalized = match['coordinates']
    
    coords_absolute = []
    for coord in coords_normalized:
        x1, y1, x2, y2 = coord
        x1 = int(x1 / 999 * image_width)
        y1 = int(y1 / 999 * image_height)
        x2 = int(x2 / 999 * image_width)
        y2 = int(y2 / 999 * image_height)
        coords_absolute.append([x1, y1, x2, y2])
    
    return label, coords_absolute
```

**ç¹ªè£½é‚Šç•Œæ¡†**:

```python
from PIL import Image, ImageDraw, ImageFont
import numpy as np

def draw_bounding_boxes(
    image: Image.Image,
    grounding_matches: List[dict],
    output_path: str = None
) -> Image.Image:
    """åœ¨åœ–ç‰‡ä¸Šç¹ªè£½é‚Šç•Œæ¡†"""
    
    image_width, image_height = image.size
    img_draw = image.copy()
    draw = ImageDraw.Draw(img_draw)
    
    # åŠé€æ˜è¦†è“‹å±¤
    overlay = Image.new('RGBA', img_draw.size, (0, 0, 0, 0))
    draw2 = ImageDraw.Draw(overlay)
    
    font = ImageFont.load_default()
    
    for match in grounding_matches:
        label, coords = extract_coordinates(match, image_width, image_height)
        
        # éš¨æ©Ÿé¡è‰²
        color = (
            np.random.randint(0, 200),
            np.random.randint(0, 200),
            np.random.randint(0, 255)
        )
        color_alpha = color + (20,)
        
        for x1, y1, x2, y2 in coords:
            # ç¹ªè£½é‚Šç•Œæ¡†
            if label == 'title':
                draw.rectangle([x1, y1, x2, y2], outline=color, width=4)
                draw2.rectangle([x1, y1, x2, y2], fill=color_alpha, width=1)
            else:
                draw.rectangle([x1, y1, x2, y2], outline=color, width=2)
                draw2.rectangle([x1, y1, x2, y2], fill=color_alpha, width=1)
            
            # ç¹ªè£½æ¨™ç±¤
            text_x, text_y = x1, max(0, y1 - 15)
            draw.text((text_x, text_y), label, font=font, fill=color)
    
    img_draw.paste(overlay, (0, 0), overlay)
    
    if output_path:
        img_draw.save(output_path)
    
    return img_draw
```

**æå–å­åœ–ç‰‡**:

```python
def extract_sub_images(
    image: Image.Image,
    grounding_matches: List[dict],
    output_dir: str = None
) -> List[dict]:
    """æå–é‚Šç•Œæ¡†å…§çš„å­åœ–ç‰‡"""
    
    image_width, image_height = image.size
    sub_images = []
    img_idx = 0
    
    for match in grounding_matches:
        label = match['label']
        
        if label == 'image':  # åªæå–æ¨™è¨˜ç‚º 'image' çš„å€åŸŸ
            label, coords = extract_coordinates(match, image_width, image_height)
            
            for x1, y1, x2, y2 in coords:
                try:
                    cropped = image.crop((x1, y1, x2, y2))
                    
                    sub_img_data = {
                        'index': img_idx,
                        'label': label,
                        'coordinates': [x1, y1, x2, y2],
                        'image': cropped
                    }
                    
                    if output_dir:
                        save_path = f"{output_dir}/sub_image_{img_idx}.jpg"
                        cropped.save(save_path)
                        sub_img_data['path'] = save_path
                    
                    sub_images.append(sub_img_data)
                    img_idx += 1
                except Exception as e:
                    print(f"Failed to extract sub-image: {e}")
    
    return sub_images
```

### 4. PDF è™•ç†æµç¨‹

```python
import fitz  # PyMuPDF
import img2pdf
from PIL import Image
import io

def pdf_to_images(
    pdf_path: str,
    dpi: int = 144
) -> List[Image.Image]:
    """å°‡ PDF è½‰æ›ç‚ºåœ–ç‰‡åˆ—è¡¨"""
    
    images = []
    pdf_document = fitz.open(pdf_path)
    
    zoom = dpi / 72.0
    matrix = fitz.Matrix(zoom, zoom)
    
    for page_num in range(pdf_document.page_count):
        page = pdf_document[page_num]
        pixmap = page.get_pixmap(matrix=matrix, alpha=False)
        
        img_data = pixmap.tobytes("png")
        img = Image.open(io.BytesIO(img_data))
        images.append(img)
    
    pdf_document.close()
    return images

def images_to_pdf(
    images: List[Image.Image],
    output_path: str
):
    """å°‡åœ–ç‰‡åˆ—è¡¨åˆä½µç‚º PDF"""
    
    image_bytes_list = []
    
    for img in images:
        if img.mode != 'RGB':
            img = img.convert('RGB')
        
        img_buffer = io.BytesIO()
        img.save(img_buffer, format='JPEG', quality=95)
        image_bytes_list.append(img_buffer.getvalue())
    
    pdf_bytes = img2pdf.convert(image_bytes_list)
    
    with open(output_path, "wb") as f:
        f.write(pdf_bytes)
```

**é‡è¤‡é é¢æª¢æ¸¬**:

```python
def is_page_repeated(output_text: str, eos_token: str = '') -> bool:
    """
    æª¢æ¸¬é é¢æ˜¯å¦å› é‡è¤‡è€Œæœªæ­£å¸¸çµæŸ
    
    é‚è¼¯:
    - å¦‚æœè¼¸å‡ºåŒ…å« EOS token: æ­£å¸¸é é¢
    - å¦‚æœæ²’æœ‰ EOS token: å¯èƒ½æ˜¯é‡è¤‡é é¢
    """
    return eos_token not in output_text

def process_pdf_with_skip(
    images: List[Image.Image],
    llm: Any,
    sampling_params: Any,
    skip_repeat: bool = True
) -> List[dict]:
    """è™•ç† PDF ä¸¦è·³éé‡è¤‡é é¢"""
    
    results = []
    
    for idx, (output, img) in enumerate(zip(outputs_list, images)):
        content = output.outputs[0].text
        
        # æª¢æŸ¥æ˜¯å¦é‡è¤‡
        if is_page_repeated(content):
            content = content.replace('', '')
            if skip_repeat:
                results.append({
                    'page_number': idx + 1,
                    'skipped': True,
                    'skip_reason': 'no_eos',
                    'text': content
                })
                continue
        
        results.append({
            'page_number': idx + 1,
            'skipped': False,
            'text': content
        })
    
    return results
```

---

## æœ€ä½³å¯¦è¸

### 1. æ¨¡å¼é¸æ“‡å»ºè­°

| å ´æ™¯ | æ¨è–¦æ¨¡å¼ | é…ç½® |
|------|---------|------|
| å°å‹æ–‡æª” (< 1MB) | `small` | crop_mode=false |
| æ¨™æº–æ–‡æª” (1-5MB) | `base` æˆ– `gundam` | crop_mode=true, max_crops=6 |
| å¤§å‹æ–‡æª” (> 5MB) | `gundam` | crop_mode=true, max_crops=6 |
| é«˜ç²¾åº¦éœ€æ±‚ | `large` | crop_mode=false |
| å¿«é€Ÿé è¦½ | `tiny` | crop_mode=false |
| è¡¨æ ¼å¯†é›† | `gundam` | ä½¿ç”¨ TABLE prompt |
| åœ–ç‰‡å¤šçš„æ–‡æª” | `gundam` | extract_sub_images=true |

### 2. æ€§èƒ½å„ªåŒ–

**æ‰¹é‡è™•ç†å„ªåŒ–**:

```python
from concurrent.futures import ThreadPoolExecutor

def preprocess_images_parallel(
    images: List[Image.Image],
    processor: DeepseekOCRProcessor,
    num_workers: int = 64
) -> List[dict]:
    """ä¸¦è¡Œé è™•ç†åœ–ç‰‡"""
    
    def process_single(image):
        return {
            "prompt": PROMPT,
            "multi_modal_data": {
                "image": processor.tokenize_with_images(
                    images=[image],
                    bos=True,
                    eos=True,
                    cropping=CROP_MODE
                )
            }
        }
    
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        batch_inputs = list(executor.map(process_single, images))
    
    return batch_inputs
```

**è¨˜æ†¶é«”ç®¡ç†**:

| GPU è¨˜æ†¶é«” | MAX_CROPS | MAX_CONCURRENCY | GPU_UTILIZATION |
|-----------|-----------|-----------------|-----------------|
| 16GB | 4 | 50 | 0.85 |
| 24GB | 6 | 100 | 0.9 |
| 32GB+ | 9 | 150 | 0.9 |

**OOM éŒ¯èª¤è™•ç†**:

```python
import torch

def handle_gpu_oom(func):
    """è£é£¾å™¨: è™•ç† GPU OOM"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except torch.cuda.OutOfMemoryError:
            # æ¸…ç†å¿«å–
            torch.cuda.empty_cache()
            
            # é™ä½ä¸¦ç™¼æ•¸
            if 'max_num_seqs' in kwargs:
                kwargs['max_num_seqs'] = max(1, kwargs['max_num_seqs'] // 2)
            
            # é‡è©¦
            return func(*args, **kwargs)
    return wrapper
```

### 3. éŒ¯èª¤è™•ç†ç¯„ä¾‹

```python
import time
from requests.exceptions import RequestException

def ocr_with_retry(
    client: DeepSeekOCRClient,
    image_path: str,
    max_retries: int = 3,
    backoff_factor: float = 2.0
):
    """å¸¶é‡è©¦æ©Ÿåˆ¶çš„ OCR"""
    
    for attempt in range(max_retries):
        try:
            return client.ocr_image(image_path=image_path)
        
        except RequestException as e:
            error_msg = str(e)
            
            if attempt == max_retries - 1:
                raise
            
            # GPU OOM: æŒ‡æ•¸é€€é¿
            if "GPU_OOM" in error_msg:
                wait_time = backoff_factor ** attempt * 5
                print(f"GPU OOM, waiting {wait_time}s...")
                time.sleep(wait_time)
            
            # Rate limit: å›ºå®šç­‰å¾…
            elif "RATE_LIMIT" in error_msg:
                time.sleep(2)
            
            # å…¶ä»–éŒ¯èª¤: ç«‹å³å¤±æ•—
            else:
                raise

# ä½¿ç”¨
try:
    result = ocr_with_retry(client, "large_document.jpg")
except Exception as e:
    print(f"Failed after retries: {e}")
```

### 4. æç¤ºè©å„ªåŒ–

**é‡å°ä¸åŒå…§å®¹é¡å‹**:

```python
PROMPT_TEMPLATES = {
    # æ–‡æª”é¡å‹
    "document": "\\nConvert the document to markdown.",
    "table": "\\nExtract table data in markdown format.",
    "form": "\\nExtract form fields and values.",
    
    # ç„¡ Grounding
    "free_ocr": "\\nFree OCR.",
    
    # ç‰¹æ®Šå…§å®¹
    "figure": "\\nParse the figure.",
    "chemistry": "\\nExtract the structural formula.",
    "geometry": "\\nExtract geometric data.",
    
    # é€šç”¨
    "general": "\\nDescribe this image in detail.",
}

# ä½¿ç”¨ç¯„ä¾‹
result = client.ocr_image(
    image_path="invoice.jpg",
    prompt=PROMPT_TEMPLATES["form"]
)
```

### 5. æ‰¹é‡è™•ç†æœ€ä½³å¯¦è¸

```python
# å¤§æ‰¹é‡è™•ç†: åˆ†æ‰¹æäº¤
def batch_ocr_large_dataset(
    image_paths: List[str],
    batch_size: int = 50
):
    """å¤§è¦æ¨¡æ‰¹é‡è™•ç†"""
    results = []
    
    for i in range(0, len(image_paths), batch_size):
        batch = image_paths[i:i+batch_size]
        
        items = [
            {
                "id": f"img_{i+j}",
                "type": "image",
                "source": path,
                "source_type": "path",
                "mode": "gundam"
            }
            for j, path in enumerate(batch)
        ]
        
        batch_result = client.batch_ocr(items, max_concurrent=20)
        results.extend(batch_result["data"]["results"])
        
        # é€²åº¦é¡¯ç¤º
        print(f"Processed {min(i+batch_size, len(image_paths))}/{len(image_paths)}")
    
    return results
```

### 6. ç‰¹æ®Šå ´æ™¯è™•ç†

**è™•ç†åŒ–å­¸çµæ§‹å¼ (SMILES)**:

```python
result = client.ocr_image(
    image_path="molecule.jpg",
    prompt="\\nExtract the structural formula."
)

# è¼¸å‡ºå¯èƒ½åŒ…å«  æ¨™ç±¤
if '' in result["data"]["text"]:
    smiles = result["data"]["text"].split('')[1].split('')[0]
    print(f"SMILES: {smiles}")
```

**è™•ç†å¹¾ä½•åœ–å½¢**:

```python
result = client.ocr_image(
    image_path="geometry.jpg",
    prompt="\\nExtract geometric data."
)

# è§£æå¹¾ä½•æ•¸æ“š
if 'Line' in result["data"]["text"]:
    geo_data = eval(result["data"]["text"])
    lines = geo_data['Line']['line']
    endpoints = geo_data['Line']['line_endpoint']
    print(f"Lines: {lines}")
```

### 7. ç›£æ§èˆ‡æ—¥èªŒ

```python
import logging
from datetime import datetime

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def ocr_with_logging(client, image_path, **kwargs):
    """å¸¶æ—¥èªŒçš„ OCR"""
    start_time = datetime.now()
    
    try:
        result = client.ocr_image(image_path=image_path, **kwargs)
        
        processing_time = result["data"]["processing_info"]["processing_time_ms"]
        
        logger.info(
            f"OCR Success | "
            f"Image: {image_path} | "
            f"Time: {processing_time:.2f}ms | "
            f"Text Length: {len(result['data']['text'])}"
        )
        
        return result
    
    except Exception as e:
        logger.error(
            f"OCR Failed | "
            f"Image: {image_path} | "
            f"Error: {str(e)}"
        )
        raise
```

---

## é™„éŒ„

### A. æ”¯æŒçš„åœ–ç‰‡æ ¼å¼

| æ ¼å¼ | æ”¯æŒ | å‚™è¨» |
|------|-----|------|
| JPEG/JPG | âœ… | æ¨è–¦ |
| PNG | âœ… | æ¨è–¦ |
| BMP | âš ï¸ | éœ€è½‰æ›ç‚º JPG |
| TIFF | âš ï¸ | éœ€è½‰æ›ç‚º JPG |
| WebP | âš ï¸ | éœ€è½‰æ›ç‚º JPG |
| PDF | âœ… | ä½¿ç”¨ PDF ç«¯é» |

### B. Token ç™½åå–®

ç”¨æ–¼ N-gram é˜²é‡è¤‡æ©Ÿåˆ¶:

| Token ID | Token | ç”¨é€” |
|----------|-------|------|
| 128821 | `<td>` | è¡¨æ ¼å–®å…ƒæ ¼é–‹å§‹ |
| 128822 | `</td>` | è¡¨æ ¼å–®å…ƒæ ¼çµæŸ |

### C. æ€§èƒ½åŸºæº–

åŸºæ–¼ NVIDIA A100 (40GB):

| æ¨¡å¼ | åœ–ç‰‡å¤§å° | å¹³å‡è™•ç†æ™‚é–“ | GPU è¨˜æ†¶é«” | ååé‡ (imgs/s) |
|------|----------|--------------|-----------|----------------|
| Tiny | 512Ã—512 | ~0.5s | ~2GB | ~2.0 |
| Small | 640Ã—640 | ~0.8s | ~3GB | ~1.2 |
| Base | 1024Ã—1024 | ~1.5s | ~5GB | ~0.7 |
| Large | 1280Ã—1280 | ~2.5s | ~8GB | ~0.4 |
| Gundam | å¯è®Š | ~2-4s | ~6-10GB | ~0.3-0.5 |

**æ‰¹é‡è™•ç†æ€§èƒ½**:
- ä¸¦ç™¼æ•¸ 50: ~15-20 imgs/s
- ä¸¦ç™¼æ•¸ 100: ~25-30 imgs/s

### D. å¸¸è¦‹å•é¡Œæ’æŸ¥

**1. GPU OOM**:
```python
# é™ä½é…ç½®
config_update = {
    "max_crops": 4,  # å¾ 6 é™åˆ° 4
    "max_concurrency": 50  # å¾ 100 é™åˆ° 50
}
client.session.put(f"{base_url}/api/v1/config", json=config_update)
```

**2. æ¨è«–è¶…æ™‚**:
```python
# å¢åŠ  timeout
client.session = requests.Session()
client.session.request = lambda *args, **kwargs: requests.request(
    *args, **{**kwargs, 'timeout': 300}
)
```

**3. é‡è¤‡é é¢éå¤š**:
```python
# èª¿æ•´ N-gram é…ç½®
config_update = {
    "ngram_config": {
        "ngram_size": 30,  # å¢åŠ  ngram_size
        "window_size": 120  # å¢åŠ  window_size
    }
}
```

### E. Docker éƒ¨ç½²

**Dockerfile**:

```dockerfile
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# å®‰è£ Python
RUN apt-get update && apt-get install -y \\
    python3.10 python3-pip git

# å®‰è£ä¾è³´
COPY requirements.txt /app/
WORKDIR /app
RUN pip3 install -r requirements.txt

# è¤‡è£½ç¨‹å¼ç¢¼
COPY . /app/

# ä¸‹è¼‰æ¨¡å‹ (å¯é¸)
# RUN python3 -c "from transformers import AutoModel; AutoModel.from_pretrained('deepseek-ai/DeepSeek-OCR')"

EXPOSE 8000

CMD ["python3", "serve_ocr.py"]
```

**docker-compose.yml**:

```yaml
version: '3.8'

services:
  deepseek-ocr:
    build: .
    ports:
      - "8000:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - MAX_CONCURRENCY=100
      - CROP_MODE=true
    volumes:
      - ./models:/app/models
      - ./outputs:/app/outputs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

**é‹è¡Œ**:

```bash
# æ§‹å»º
docker-compose build

# å•Ÿå‹•
docker-compose up -d

# æŸ¥çœ‹æ—¥èªŒ
docker-compose logs -f

# æ¸¬è©¦
curl http://localhost:8000/api/v1/health
```

---

## ç‰ˆæœ¬è®Šæ›´æ—¥èªŒ

### v2.0.0 (2025-11-04)
- âœ… æ–°å¢æµå¼è¼¸å‡ºæ”¯æŒ
- âœ… æ–°å¢ Grounding è¦–è¦ºå®šä½åŠŸèƒ½
- âœ… æ–°å¢ PDF æ‰¹é‡è™•ç†
- âœ… æ–°å¢ N-gram é˜²é‡è¤‡æ©Ÿåˆ¶
- âœ… å„ªåŒ–å‹•æ…‹è£åˆ‡æ¼”ç®—æ³•
- âœ… å®Œæ•´çš„éŒ¯èª¤è™•ç†é«”ç³»

### v1.0.0 (2025-11-01)
- âœ… åˆå§‹ç‰ˆæœ¬
- âœ… åŸºç¤åœ–ç‰‡ OCR
- âœ… å¤šæ¨¡å¼æ”¯æŒ

---

## ç¶­è­·èˆ‡æ”¯æŒ

**ç¶­è­·è€…**: Yueh-Chun Hsieh  
**è¯çµ¡æ–¹å¼**: ocr-support@example.com  
**æ–‡æª”å€‰åº«**: https://github.com/your-org/deepseek-ocr-api  
**å•é¡Œè¿½è¹¤**: https://github.com/your-org/deepseek-ocr-api/issues  

**æŠ€è¡“æ”¯æŒ**:
- Slack: #deepseek-ocr
- Email: wilson5711704@gmail.com
- æ–‡æª”: https://docs.example.com/deepseek-ocr

---

**æœ€å¾Œæ›´æ–°**: 2025-11-04  
**æ–‡æª”ç‰ˆæœ¬**: 2.0.0
    "